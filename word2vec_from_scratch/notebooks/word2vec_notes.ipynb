{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "FAx1xjXiwNfd"
      },
      "outputs": [],
      "source": [
        "########################CONSTANTS######################################\n",
        "CBOW_N_WORDS = 4\n",
        "\n",
        "MIN_WORD_FREQUENCY = 50\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "EMBED_DIMENSION = 300\n",
        "EMBED_MAX_NORM = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = set([\n",
        "    'i', 'me', 'my', 'frequent','myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n",
        "    \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
        "    'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
        "    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
        "    'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
        "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
        "    'having', 'do', 'does','sometimes', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
        "    'or', 'because', 'as','lastly', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
        "    'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
        "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
        "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
        "    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
        "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
        "    'very', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
        "    'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n",
        "    \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
        "    \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n",
        "    'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n",
        "    \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn',\n",
        "    \"wouldn't\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "lKwbP-cQwa86"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################DATALOADER##################################\n",
        "import torch\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\"\"\"from constants import\n",
        " (\n",
        "    CBOW_N_WORDS,\n",
        "    MIN_WORD_FREQUENCY,\n",
        "    MAX_SEQUENCE_LENGTH,\n",
        ")\"\"\"\n",
        "\n",
        "class text_dataset(Dataset):\n",
        "  def __init__(self, filepath, text_column=\"text\"):\n",
        "      df = pd.read_pickle(filepath)\n",
        "      self.samples = df[text_column].tolist()\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.samples[idx]\n",
        "  def __len__(self):\n",
        "      return len(self.samples)\n",
        "\n",
        "\n",
        "def data_iterator(data_dir, filename, text_column=\"text\"):\n",
        "    filepath = os.path.join(data_dir, filename)\n",
        "    dataset = text_dataset(filepath, text_column)\n",
        "    return dataset\n",
        "\n",
        "def tokenizer(text:str):\n",
        "\n",
        "  #stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "  text = re.sub(r\"</?s>|\\[/?INST\\]\", \"\", text)\n",
        "  text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "  cleaned_text = text.lower()\n",
        "  words = cleaned_text.split()\n",
        "  words = [word for word in words if word not in english_stopwords]\n",
        "  return words\n",
        "\n",
        "\n",
        "def build_vocab(data_iter, tokenizer):\n",
        "\n",
        "    sentences = map(tokenizer, data_iter)\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    vocab_words = [word for word, freq in word_counts.items() if freq >= MIN_WORD_FREQUENCY]\n",
        "    #from word get ID\n",
        "    word2id = {word: idx for idx, word in enumerate(vocab_words, start=1)}\n",
        "    word2id[\"<unk>\"] = 0\n",
        "    #from ID get word\n",
        "    id2word = {idx: word for word, idx in word2id.items()}\n",
        "    return word2id,id2word\n",
        "\n",
        "def collate_cbow(batch, text_pipeline):\n",
        "  ##batch is a list of text paragraph\n",
        "    batch_input, batch_output = [], []\n",
        "    for text in batch:\n",
        "      tokens_IDs = text_pipeline(text) # return a list of the words IDS\n",
        "      if len(tokens_IDs) < CBOW_N_WORDS * 2 + 1:\n",
        "          continue\n",
        "      if MAX_SEQUENCE_LENGTH:\n",
        "            tokens_IDs = tokens_IDs[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "      for idx in range(len(tokens_IDs) - CBOW_N_WORDS * 2):\n",
        "            token_id_sequence = tokens_IDs[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
        "            output = token_id_sequence.pop(CBOW_N_WORDS)\n",
        "            input_ = token_id_sequence\n",
        "            batch_input.append(input_)\n",
        "            batch_output.append(output)\n",
        "\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "    return batch_input, batch_output\n",
        "\n",
        "def get_dataloader_word2id_id2word(filename,data_dir, batch_size, shuffle,word2id=None):\n",
        "\n",
        "  data_itr   = data_iterator(data_dir, filename)\n",
        "  tokenizer_ = tokenizer\n",
        "  if not word2id:\n",
        "    word2id,id2word = build_vocab(data_itr,tokenizer_)\n",
        "  text_pipeline = lambda x: [word2id.get(word, 0) for word in tokenizer(x)]\n",
        "  collate_fn = collate_cbow\n",
        "  dataloader = DataLoader(\n",
        "        data_itr,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
        "    )\n",
        "  return dataloader, word2id, id2word"
      ],
      "metadata": {
        "id": "qyiZYYuRwhGd"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hedP7tZTdRrT"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udvyE3f2dRqG"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pR-zbFo4FhWs"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################MODEL#####################################\n",
        "import torch.nn as nn\n",
        "\"\"\"from constants import EMBED_DIMENSION, EMBED_MAX_NORM\"\"\"\n",
        "class CBOW(nn.Module):\n",
        "      def __init__(self, vocab_size: int):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.Embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=EMBED_DIMENSION,max_norm=EMBED_MAX_NORM)\n",
        "        self.Outputs=nn.Linear(in_features=EMBED_DIMENSION,out_features=vocab_size)\n",
        "      def forward(self,inputs):\n",
        "        weights = self.Embedding(inputs)\n",
        "        weights = weights.mean(axis=1)\n",
        "        outputs = self.Outputs(weights)\n",
        "        return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "9vNjVVI1dRmS"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################Helpers###############################################\n",
        "import yaml\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "#from Utils.model import CBOW_Model\n",
        "\n",
        "def get_model_class(model_name: str):\n",
        "    if model_name == \"CBOW\":\n",
        "        return CBOW\n",
        "    else:\n",
        "      raise ValueError(\"Only Available model is CBOW\")\n",
        "      return\n",
        "\n",
        "def get_optimizer_class(optimizer:str):\n",
        "   if optimizer == 'Adam':\n",
        "        return optim.Adam\n",
        "   else:\n",
        "      raise ValueError(\"Only Available optimizer is Adam\")\n",
        "      return\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j1JooaIs8reb"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## Trainer #########################\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, epochs, train_dataloader, val_dataloader, train_steps, val_steps, optimizer,device):\n",
        "        self.epochs = epochs                      # from config file\n",
        "        self.train_dataloader = train_dataloader  # from class dataloader\n",
        "        self.val_dataloader = val_dataloader      # from class dataloader\n",
        "        self.train_steps = train_steps            # from config file\n",
        "        self.val_steps = val_steps                # from config file\n",
        "        self.model = model                        # create instance in train.py\n",
        "        self.optimizer = optimizer                # create instance in train.py\n",
        "        self.device = device                      #checked in the train.py\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()    # fully defined\n",
        "        self.model.to(self.device)                # adjust the model to the device\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "\n",
        "    def training(self):\n",
        "\n",
        "      for epoch in range(self.epochs):\n",
        "        self.train_epoch()\n",
        "        self.val_epoch()\n",
        "        print(f'Epoch:{epoch + 1}/{self.epochs},train_loss = {self.loss[\"train\"][-1]:.4f} , val_loss = {self.loss[\"val\"][-1]:.4f}')\n",
        "      print('training has been completed :)')\n",
        "\n",
        "    def train_epoch(self):\n",
        "\n",
        "        running_loss = []\n",
        "        self.model.train()\n",
        "\n",
        "        for step , batch in enumerate (self.train_dataloader,start=1):\n",
        "\n",
        "          #selecting batch & adjust to device\n",
        "          inputs,targets  = batch\n",
        "          inputs,targets  = inputs.to(self.device) , targets.to(self.device)\n",
        "\n",
        "          #forward propagation\n",
        "          self.optimizer.zero_grad()\n",
        "          outputs_pred = self.model(inputs)\n",
        "          loss  = self.criterion(outputs_pred,targets)\n",
        "\n",
        "          #backward propagation\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "\n",
        "\n",
        "          running_loss.append(loss.item())\n",
        "\n",
        "          if step == self.train_steps:\n",
        "            break\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"train\"].append(epoch_loss)\n",
        "\n",
        "    def val_epoch(self):\n",
        "\n",
        "        running_loss = []\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.inference_mode(): # turns off gradient tracking\n",
        "\n",
        "          for step , batch in enumerate(self.val_dataloader,start=1):\n",
        "\n",
        "            #selecting batch & adjust to device\n",
        "            inputs,targets  = batch\n",
        "            inputs,targets  = inputs.to(self.device) , targets.to(self.device)\n",
        "\n",
        "            #forward propagation\n",
        "            outputs_pred = self.model(inputs)\n",
        "            loss  = self.criterion(outputs_pred,targets)\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            if step == self.val_steps:\n",
        "              break\n",
        "\n",
        "          epoch_loss = np.mean(running_loss)\n",
        "          self.loss[\"val\"].append(epoch_loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cAzRl5NsdRk6"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################Train################################\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from utils.trainer import Trainer\n",
        "\"\"\"from utils.helper import (\n",
        "    get_model_class,\n",
        "    get_optimizer_class,\n",
        "    get_lr_scheduler,\n",
        "    save_config,\n",
        "    save_vocab,\n",
        ")\"\"\"\n",
        "#from Utils import get_dataloader_word2id_id2word\n",
        "\n",
        "#reading config file\n",
        "with open(\"config.yaml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "#creating batches and get size of our vocab\n",
        "\n",
        "train_dataloader, word2id , id2word = get_dataloader_word2id_id2word(filename=config['train_dataset'],\n",
        "                                                               data_dir=config['data_dir'],\n",
        "                                                               batch_size=config['train_batch_size'],\n",
        "                                                               shuffle=config['shuffle'])\n",
        "val_dataloader, _ , _ = get_dataloader_word2id_id2word(filename=config['val_dataset'],\n",
        "                                                               data_dir=config['data_dir'],\n",
        "                                                               batch_size=config['val_batch_size'],\n",
        "                                                               shuffle=config['shuffle'])\n",
        "vocab = len(word2id)\n",
        "\n",
        "#create instances\n",
        "\n",
        "model_class = get_model_class(config['model_name'])\n",
        "model = model_class(vocab_size=vocab)\n",
        "\n",
        "optimizer_class = get_optimizer_class(config['optimizer'])\n",
        "optimizer = optimizer_class(params= model.parameters() ,lr=config['learning_rate'])\n",
        "\n",
        "# select the deivce\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# create instance of the trian class\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    device = device,\n",
        "    epochs = config['epochs'],\n",
        "    train_dataloader = train_dataloader,\n",
        "    val_dataloader   = val_dataloader,\n",
        "    train_steps = config['train_steps'],\n",
        "    val_steps   = config['val_steps'],\n",
        "    optimizer= optimizer\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "XbQBwPSWw4pK"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEZLtkYWGvIB",
        "outputId": "5c87d136-c7e7-4725-c8c5-f1e0c0ec5533"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1/50,train_loss = 6.0427 , val_loss = 5.8591\n",
            "Epoch:2/50,train_loss = 5.7652 , val_loss = 5.4598\n",
            "Epoch:3/50,train_loss = 5.3850 , val_loss = 5.0446\n",
            "Epoch:4/50,train_loss = 5.0702 , val_loss = 4.7246\n",
            "Epoch:5/50,train_loss = 4.8620 , val_loss = 4.4536\n",
            "Epoch:6/50,train_loss = 4.7139 , val_loss = 4.2488\n",
            "Epoch:7/50,train_loss = 4.6023 , val_loss = 4.0947\n",
            "Epoch:8/50,train_loss = 4.5330 , val_loss = 3.9727\n",
            "Epoch:9/50,train_loss = 4.4701 , val_loss = 3.9255\n",
            "Epoch:10/50,train_loss = 4.4399 , val_loss = 3.8875\n",
            "Epoch:11/50,train_loss = 4.3974 , val_loss = 3.8381\n",
            "Epoch:12/50,train_loss = 4.3692 , val_loss = 3.8247\n",
            "Epoch:13/50,train_loss = 4.3399 , val_loss = 3.8232\n",
            "Epoch:14/50,train_loss = 4.3096 , val_loss = 3.8072\n",
            "Epoch:15/50,train_loss = 4.2848 , val_loss = 3.8041\n",
            "Epoch:16/50,train_loss = 4.2619 , val_loss = 3.7958\n",
            "Epoch:17/50,train_loss = 4.2409 , val_loss = 3.7779\n",
            "Epoch:18/50,train_loss = 4.2211 , val_loss = 3.7843\n",
            "Epoch:19/50,train_loss = 4.1969 , val_loss = 3.7835\n",
            "Epoch:20/50,train_loss = 4.1865 , val_loss = 3.7797\n",
            "Epoch:21/50,train_loss = 4.1714 , val_loss = 3.7731\n",
            "Epoch:22/50,train_loss = 4.1590 , val_loss = 3.7675\n",
            "Epoch:23/50,train_loss = 4.1409 , val_loss = 3.7507\n",
            "Epoch:24/50,train_loss = 4.1242 , val_loss = 3.7661\n",
            "Epoch:25/50,train_loss = 4.1069 , val_loss = 3.7440\n",
            "Epoch:26/50,train_loss = 4.0927 , val_loss = 3.7634\n",
            "Epoch:27/50,train_loss = 4.0754 , val_loss = 3.7442\n",
            "Epoch:28/50,train_loss = 4.0670 , val_loss = 3.7635\n",
            "Epoch:29/50,train_loss = 4.0574 , val_loss = 3.7545\n",
            "Epoch:30/50,train_loss = 4.0248 , val_loss = 3.7566\n",
            "Epoch:31/50,train_loss = 4.0340 , val_loss = 3.7584\n",
            "Epoch:32/50,train_loss = 4.0237 , val_loss = 3.7508\n",
            "Epoch:33/50,train_loss = 4.0120 , val_loss = 3.7534\n",
            "Epoch:34/50,train_loss = 4.0017 , val_loss = 3.7527\n",
            "Epoch:35/50,train_loss = 3.9880 , val_loss = 3.7439\n",
            "Epoch:36/50,train_loss = 3.9770 , val_loss = 3.7375\n",
            "Epoch:37/50,train_loss = 3.9690 , val_loss = 3.7711\n",
            "Epoch:38/50,train_loss = 3.9534 , val_loss = 3.7691\n",
            "Epoch:39/50,train_loss = 3.9469 , val_loss = 3.7613\n",
            "Epoch:40/50,train_loss = 3.9391 , val_loss = 3.7537\n",
            "Epoch:41/50,train_loss = 3.9347 , val_loss = 3.7509\n",
            "Epoch:42/50,train_loss = 3.9301 , val_loss = 3.7513\n",
            "Epoch:43/50,train_loss = 3.9137 , val_loss = 3.7639\n",
            "Epoch:44/50,train_loss = 3.9083 , val_loss = 3.7584\n",
            "Epoch:45/50,train_loss = 3.9022 , val_loss = 3.7596\n",
            "Epoch:46/50,train_loss = 3.9159 , val_loss = 3.7603\n",
            "Epoch:47/50,train_loss = 3.8893 , val_loss = 3.7615\n",
            "Epoch:48/50,train_loss = 3.8679 , val_loss = 3.7675\n",
            "Epoch:49/50,train_loss = 3.8648 , val_loss = 3.7698\n",
            "Epoch:50/50,train_loss = 3.8620 , val_loss = 3.7766\n",
            "training has been completed :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "model.Embedding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8M72WeUuT_y",
        "outputId": "6b792d58-2ddd-43f6-9fcc-94028a70e088"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(463, 300, max_norm=1)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(word):\n",
        "    idx = word2id.get(word.lower(),0)\n",
        "    if idx == 0:\n",
        "        raise ValueError(\"OOV(out of vocabulary!!)\")\n",
        "        return\n",
        "    return model.Embedding.weight[idx].detach()\n",
        "\n",
        "def find_similar_words(word, top_n=10):\n",
        "    vector = get_vector(word)\n",
        "    if vector is None:\n",
        "        return None\n",
        "    all_vectors = model.Embedding.weight.detach()\n",
        "    similarities = F.cosine_similarity(vector.unsqueeze(0), all_vectors)\n",
        "    top_indices = torch.topk(similarities, top_n + 1).indices.tolist()\n",
        "\n",
        "    results = []\n",
        "    for i in top_indices:\n",
        "        candidate = id2word[i]\n",
        "        if candidate != word:\n",
        "            similarity_score = similarities[i].item()\n",
        "            similarity_percent = round(similarity_score * 100, 2)\n",
        "            results.append((candidate, similarity_percent))\n",
        "        if len(results) == top_n:\n",
        "            break\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "hKkO0UQ7jZqk"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = get_vector('finger')\n",
        "v2 = get_vector('leg')"
      ],
      "metadata": {
        "id": "9hKw54VtenMJ"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar_words('periods')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AsB1IWxlecA",
        "outputId": "29dbea30-0ad2-4faa-df3e-b04e5c44b2e5"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('menstruation', 92.17),\n",
              " ('menstrual', 92.02),\n",
              " ('legs', 91.88),\n",
              " ('bleeding', 91.78),\n",
              " ('heavy', 90.67),\n",
              " ('hot', 89.93),\n",
              " ('low', 89.7),\n",
              " ('ache', 89.16),\n",
              " ('urination', 89.05),\n",
              " ('even', 88.96)]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar_words('relieve')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILHTdurt7FHp",
        "outputId": "2aa8539c-3744-4be8-e691-254a37ad2c32"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('help', 92.11),\n",
              " ('reduce', 88.88),\n",
              " ('use', 84.1),\n",
              " ('manage', 82.66),\n",
              " ('alleviate', 81.73),\n",
              " ('prevent', 81.57),\n",
              " ('used', 81.33),\n",
              " ('acid', 81.05),\n",
              " ('using', 80.35),\n",
              " ('topical', 80.19)]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar_words('cardiac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoa5hmbC7E3I",
        "outputId": "62b75900-167a-49da-b673-14fb12ac7739"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('electrocardiogram', 93.35),\n",
              " ('check', 89.15),\n",
              " ('levels', 87.04),\n",
              " ('measure', 86.58),\n",
              " ('monitor', 84.56),\n",
              " ('intravenous', 83.75),\n",
              " ('panel', 83.42),\n",
              " ('test', 83.04),\n",
              " ('lipid', 82.81),\n",
              " ('rule', 82.71)]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar_words('vomiting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2dtHvAu7E1G",
        "outputId": "cdcf2641-ad56-4e5b-9484-3b7688de2cb7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nausea', 96.45),\n",
              " ('sharp', 95.56),\n",
              " ('chest', 94.95),\n",
              " ('lower', 94.87),\n",
              " ('diarrhea', 94.8),\n",
              " ('heartburn', 94.65),\n",
              " ('abdominal', 93.71),\n",
              " ('fatigue', 93.71),\n",
              " ('upper', 93.02),\n",
              " ('burning', 92.89)]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"word2vec_model.pth\")"
      ],
      "metadata": {
        "id": "EifPRn3gkabX"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "LrkZFOPXmm02"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(word2id.items(), columns=['Word', 'ID'])\n"
      ],
      "metadata": {
        "id": "_QhMsAWxmmoQ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = pd.DataFrame(id2word.items(), columns=['ID','Word'])\n"
      ],
      "metadata": {
        "id": "Dg2yGxXCncdz"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2.to_pickle('id2word')"
      ],
      "metadata": {
        "id": "PqgcEpFJmmmY"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1egZPs--kRCq",
        "outputId": "f80b35d2-804e-485c-e4f8-8f2ec52ed177"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'doctor',\n",
              " 2: 'experiencing',\n",
              " 3: 'symptoms',\n",
              " 4: 'lately',\n",
              " 5: 'pain',\n",
              " 6: 'hip',\n",
              " 7: 'skin',\n",
              " 8: 'rash',\n",
              " 9: 'problems',\n",
              " 10: 'face',\n",
              " 11: 'could',\n",
              " 12: 'causing',\n",
              " 13: 'based',\n",
              " 14: 's',\n",
              " 15: 'possible',\n",
              " 16: 'disease',\n",
              " 17: 'time',\n",
              " 18: 'recently',\n",
              " 19: 'fever',\n",
              " 20: 'shoulder',\n",
              " 21: 'dizziness',\n",
              " 22: 'eye',\n",
              " 23: 'happening',\n",
              " 24: 'may',\n",
              " 25: 'syndrome',\n",
              " 26: 'condition',\n",
              " 27: 'body',\n",
              " 28: 'like',\n",
              " 29: 'examination',\n",
              " 30: 'issues',\n",
              " 31: 'noticed',\n",
              " 32: 'blood',\n",
              " 33: 'urine',\n",
              " 34: 'vomiting',\n",
              " 35: 'testicles',\n",
              " 36: 'kidneys',\n",
              " 37: 'causes',\n",
              " 38: 'need',\n",
              " 39: 'run',\n",
              " 40: 'tests',\n",
              " 41: 'confirm',\n",
              " 42: 'diagnosis',\n",
              " 43: 'trouble',\n",
              " 44: 'feels',\n",
              " 45: 'lungs',\n",
              " 46: 't',\n",
              " 47: 'properly',\n",
              " 48: 'related',\n",
              " 49: 'disorder',\n",
              " 50: 'worried',\n",
              " 51: 'high',\n",
              " 52: 'pressure',\n",
              " 53: 'take',\n",
              " 54: 'check',\n",
              " 55: 'series',\n",
              " 56: 'medical',\n",
              " 57: 'assess',\n",
              " 58: 'hematologic',\n",
              " 59: 'complete',\n",
              " 60: 'count',\n",
              " 61: 'cbc',\n",
              " 62: 'lipid',\n",
              " 63: 'panel',\n",
              " 64: 'evaluate',\n",
              " 65: 'levels',\n",
              " 66: 'glucose',\n",
              " 67: 'measurement',\n",
              " 68: 'level',\n",
              " 69: 'electrocardiogram',\n",
              " 70: 'look',\n",
              " 71: 'heart',\n",
              " 72: 'hemoglobin',\n",
              " 73: 'c',\n",
              " 74: 'test',\n",
              " 75: 'finally',\n",
              " 76: 'ultrasonography',\n",
              " 77: 'ultrasound',\n",
              " 78: 'kidney',\n",
              " 79: 'function',\n",
              " 80: 'shortness',\n",
              " 81: 'breath',\n",
              " 82: 'coughing',\n",
              " 83: 'know',\n",
              " 84: 'yes',\n",
              " 85: 'sounds',\n",
              " 86: 'experience',\n",
              " 87: 'sharp',\n",
              " 88: 'abdominal',\n",
              " 89: 'chest',\n",
              " 90: 'problem',\n",
              " 91: 'seems',\n",
              " 92: 'might',\n",
              " 93: 'abdomen',\n",
              " 94: 'caused',\n",
              " 95: 'injury',\n",
              " 96: 'also',\n",
              " 97: 'due',\n",
              " 98: 'throat',\n",
              " 99: 'swelling',\n",
              " 100: 'sore',\n",
              " 101: 'difficulty',\n",
              " 102: 'foreign',\n",
              " 103: 'imaging',\n",
              " 104: 'immediately',\n",
              " 105: 'hi',\n",
              " 106: 'think',\n",
              " 107: 'foot',\n",
              " 108: 'really',\n",
              " 109: 'itchy',\n",
              " 110: 'redness',\n",
              " 111: 'physical',\n",
              " 112: 'exam',\n",
              " 113: 'performed',\n",
              " 114: 'required',\n",
              " 115: 'determine',\n",
              " 116: 'extent',\n",
              " 117: 'infection',\n",
              " 118: 'depending',\n",
              " 119: 'severity',\n",
              " 120: 'wound',\n",
              " 121: 'care',\n",
              " 122: 'management',\n",
              " 123: 'necessary',\n",
              " 124: 'prevent',\n",
              " 125: 'complications',\n",
              " 126: 'consider',\n",
              " 127: 'excision',\n",
              " 128: 'removal',\n",
              " 129: 'infected',\n",
              " 130: 'area',\n",
              " 131: 'order',\n",
              " 132: 'monitor',\n",
              " 133: 'health',\n",
              " 134: 'well',\n",
              " 135: 'screen',\n",
              " 136: 'diabetes',\n",
              " 137: 'depression',\n",
              " 138: 'ensure',\n",
              " 139: 'emotional',\n",
              " 140: 'patient',\n",
              " 141: 'leg',\n",
              " 142: 'breathing',\n",
              " 143: 'weeks',\n",
              " 144: 'wrong',\n",
              " 145: 'suffering',\n",
              " 146: 'looks',\n",
              " 147: 'abnormal',\n",
              " 148: 'viral',\n",
              " 149: 'lead',\n",
              " 150: 'low',\n",
              " 151: 'back',\n",
              " 152: 'growth',\n",
              " 153: 'urination',\n",
              " 154: 'cases',\n",
              " 155: 'cause',\n",
              " 156: 'urinary',\n",
              " 157: 'would',\n",
              " 158: 'recommend',\n",
              " 159: 'cramps',\n",
              " 160: 'spasms',\n",
              " 161: 'painful',\n",
              " 162: 'important',\n",
              " 163: 'get',\n",
              " 164: 'start',\n",
              " 165: 'treatment',\n",
              " 166: 'persistent',\n",
              " 167: 'past',\n",
              " 168: 'days',\n",
              " 169: 'idea',\n",
              " 170: 'case',\n",
              " 171: 'going',\n",
              " 172: 'prescribe',\n",
              " 173: 'medication',\n",
              " 174: 'help',\n",
              " 175: 'alleviate',\n",
              " 176: 'acid',\n",
              " 177: 'topical',\n",
              " 178: 'product',\n",
              " 179: 'hurts',\n",
              " 180: 'course',\n",
              " 181: 'antibiotics',\n",
              " 182: 'suggest',\n",
              " 183: 'excessive',\n",
              " 184: 'diagnostic',\n",
              " 185: 'procedures',\n",
              " 186: 'interview',\n",
              " 187: 'consultation',\n",
              " 188: 'therapeutic',\n",
              " 189: 'nervous',\n",
              " 190: 'system',\n",
              " 191: 'however',\n",
              " 192: 'underlying',\n",
              " 193: 'perform',\n",
              " 194: 'prostate',\n",
              " 195: 'specific',\n",
              " 196: 'eyes',\n",
              " 197: 'corneal',\n",
              " 198: 'suspect',\n",
              " 199: 'something',\n",
              " 200: 'bones',\n",
              " 201: 'stiffness',\n",
              " 202: 'knee',\n",
              " 203: 'bone',\n",
              " 204: 'arthritis',\n",
              " 205: 'vaginal',\n",
              " 206: 'discharge',\n",
              " 207: 'bleeding',\n",
              " 208: 'pregnancy',\n",
              " 209: 'please',\n",
              " 210: 'tell',\n",
              " 211: 'described',\n",
              " 212: 'nasal',\n",
              " 213: 'congestion',\n",
              " 214: 'likely',\n",
              " 215: 'right',\n",
              " 216: 'away',\n",
              " 217: 'headache',\n",
              " 218: 'loss',\n",
              " 219: 'sensation',\n",
              " 220: 'medications',\n",
              " 221: 'feel',\n",
              " 222: 'swollen',\n",
              " 223: 'chronic',\n",
              " 224: 'behavior',\n",
              " 225: 'drug',\n",
              " 226: 'abuse',\n",
              " 227: 'alcohol',\n",
              " 228: 'seek',\n",
              " 229: 'discuss',\n",
              " 230: 'options',\n",
              " 231: 'facial',\n",
              " 232: 'vision',\n",
              " 233: 'diminished',\n",
              " 234: 'diagnosed',\n",
              " 235: 'vitamin',\n",
              " 236: 'sodium',\n",
              " 237: 'lot',\n",
              " 238: 'neck',\n",
              " 239: 'told',\n",
              " 240: 'taking',\n",
              " 241: 'used',\n",
              " 242: 'manage',\n",
              " 243: 'include',\n",
              " 244: 'type',\n",
              " 245: 'vaccine',\n",
              " 246: 'best',\n",
              " 247: 'peripheral',\n",
              " 248: 'edema',\n",
              " 249: 'occurs',\n",
              " 250: 'become',\n",
              " 251: 'fluid',\n",
              " 252: 'retention',\n",
              " 253: 'come',\n",
              " 254: 'plan',\n",
              " 255: 'legs',\n",
              " 256: 'several',\n",
              " 257: 'including',\n",
              " 258: 'testing',\n",
              " 259: 'feeling',\n",
              " 260: 'weak',\n",
              " 261: 'stomach',\n",
              " 262: 'poisoning',\n",
              " 263: 'see',\n",
              " 264: 'urinalysis',\n",
              " 265: 'intravenous',\n",
              " 266: 'replacement',\n",
              " 267: 'electrolytes',\n",
              " 268: 'measure',\n",
              " 269: 'better',\n",
              " 270: 'nausea',\n",
              " 271: 'allergic',\n",
              " 272: 'reaction',\n",
              " 273: 'mentioned',\n",
              " 274: 'common',\n",
              " 275: 'side',\n",
              " 276: 'quite',\n",
              " 277: 'first',\n",
              " 278: 'mental',\n",
              " 279: 'behind',\n",
              " 280: 'additionally',\n",
              " 281: 'use',\n",
              " 282: 'therapy',\n",
              " 283: 'tired',\n",
              " 284: 'make',\n",
              " 285: 'proper',\n",
              " 286: 'conduct',\n",
              " 287: 'okay',\n",
              " 288: 'tightness',\n",
              " 289: 'inflammation',\n",
              " 290: 'muscle',\n",
              " 291: 'toe',\n",
              " 292: 'joint',\n",
              " 293: 'associated',\n",
              " 294: 'sure',\n",
              " 295: 'magnetic',\n",
              " 296: 'resonance',\n",
              " 297: 'mri',\n",
              " 298: 'scan',\n",
              " 299: 'spinal',\n",
              " 300: 'lesion',\n",
              " 301: 'often',\n",
              " 302: 'serious',\n",
              " 303: 'seem',\n",
              " 304: 'control',\n",
              " 305: 'fainting',\n",
              " 306: 'result',\n",
              " 307: 'ache',\n",
              " 308: 'cough',\n",
              " 309: 'leading',\n",
              " 310: 'discomfort',\n",
              " 311: 'symptom',\n",
              " 312: 'weakness',\n",
              " 313: 'getting',\n",
              " 314: 'abnormalities',\n",
              " 315: 'damage',\n",
              " 316: 'along',\n",
              " 317: 'exercises',\n",
              " 318: 'rule',\n",
              " 319: 'unusual',\n",
              " 320: 'itching',\n",
              " 321: 'heartburn',\n",
              " 322: 'reason',\n",
              " 323: 'dizzy',\n",
              " 324: 'nauseous',\n",
              " 325: 'attention',\n",
              " 326: 'bladder',\n",
              " 327: 'cancer',\n",
              " 328: 'biopsy',\n",
              " 329: 'x',\n",
              " 330: 'ray',\n",
              " 331: 'computed',\n",
              " 332: 'tomography',\n",
              " 333: 'ct',\n",
              " 334: 'rectal',\n",
              " 335: 'combination',\n",
              " 336: 'mouth',\n",
              " 337: 'lump',\n",
              " 338: 'mass',\n",
              " 339: 'cells',\n",
              " 340: 'ear',\n",
              " 341: 'issue',\n",
              " 342: 'known',\n",
              " 343: 'hello',\n",
              " 344: 'hand',\n",
              " 345: 'finger',\n",
              " 346: 'arm',\n",
              " 347: 'decreased',\n",
              " 348: 'appetite',\n",
              " 349: 'fatigue',\n",
              " 350: 'sorry',\n",
              " 351: 'genetic',\n",
              " 352: 'affects',\n",
              " 353: 'baby',\n",
              " 354: 'parts',\n",
              " 355: 'various',\n",
              " 356: 'irritation',\n",
              " 357: 'irregular',\n",
              " 358: 'radiographic',\n",
              " 359: 'procedure',\n",
              " 360: 'one',\n",
              " 361: 'affected',\n",
              " 362: 'anemia',\n",
              " 363: 'lower',\n",
              " 364: 'red',\n",
              " 365: 'developed',\n",
              " 366: 'spots',\n",
              " 367: 'treat',\n",
              " 368: 'even',\n",
              " 369: 'using',\n",
              " 370: 'reduce',\n",
              " 371: 'provide',\n",
              " 372: 'plain',\n",
              " 373: 'organs',\n",
              " 374: 'elbow',\n",
              " 375: 'ankle',\n",
              " 376: 'called',\n",
              " 377: 'fingers',\n",
              " 378: 'strange',\n",
              " 379: 'deficiency',\n",
              " 380: 'around',\n",
              " 381: 'cold',\n",
              " 382: 'liver',\n",
              " 383: 'tight',\n",
              " 384: 'rate',\n",
              " 385: 'stool',\n",
              " 386: 'tissue',\n",
              " 387: 'scrotum',\n",
              " 388: 'undergo',\n",
              " 389: 'burning',\n",
              " 390: 'white',\n",
              " 391: 'pelvis',\n",
              " 392: 'bacterial',\n",
              " 393: 'examine',\n",
              " 394: 'headaches',\n",
              " 395: 'require',\n",
              " 396: 'open',\n",
              " 397: 'severe',\n",
              " 398: 'hear',\n",
              " 399: 'hot',\n",
              " 400: 'conditions',\n",
              " 401: 'involuntary',\n",
              " 402: 'movements',\n",
              " 403: 'wrist',\n",
              " 404: 'ophthalmic',\n",
              " 405: 'infections',\n",
              " 406: 'diagnose',\n",
              " 407: 'dry',\n",
              " 408: 'rare',\n",
              " 409: 'diarrhea',\n",
              " 410: 'depressive',\n",
              " 411: 'movement',\n",
              " 412: 'soon',\n",
              " 413: 'psychotic',\n",
              " 414: 'evaluation',\n",
              " 415: 'breast',\n",
              " 416: 'appears',\n",
              " 417: 'appearing',\n",
              " 418: 'eyelids',\n",
              " 419: 'pelvic',\n",
              " 420: 'periods',\n",
              " 421: 'heavy',\n",
              " 422: 'menstrual',\n",
              " 423: 'flow',\n",
              " 424: 'addition',\n",
              " 425: 'healing',\n",
              " 426: 'small',\n",
              " 427: 'us',\n",
              " 428: 'menstruation',\n",
              " 429: 'anxiety',\n",
              " 430: 'lesions',\n",
              " 431: 'appropriate',\n",
              " 432: 'muscles',\n",
              " 433: 'history',\n",
              " 434: 'weight',\n",
              " 435: 'nose',\n",
              " 436: 'acne',\n",
              " 437: 'changes',\n",
              " 438: 'nerve',\n",
              " 439: 'improve',\n",
              " 440: 'constipation',\n",
              " 441: 'upper',\n",
              " 442: 'night',\n",
              " 443: 'feet',\n",
              " 444: 'gain',\n",
              " 445: 'brain',\n",
              " 446: 'fracture',\n",
              " 447: 'relieve',\n",
              " 448: 'hands',\n",
              " 449: 'acute',\n",
              " 450: 'eyelid',\n",
              " 451: 'keep',\n",
              " 452: 'develop',\n",
              " 453: 'memory',\n",
              " 454: 'hearing',\n",
              " 455: 'speaking',\n",
              " 456: 'seizures',\n",
              " 457: 'thyroid',\n",
              " 458: 'cardiac',\n",
              " 459: 'groin',\n",
              " 460: 'head',\n",
              " 461: 'swallowing',\n",
              " 462: 'cyst',\n",
              " 0: '<unk>'}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Assume you already have these loaded:\n",
        "# - model\n",
        "# - word2id\n",
        "# - id2word\n",
        "\n",
        "# --- Function to get the vector for a word\n",
        "def get_vector(word):\n",
        "    idx = word2id.get(word.lower(), 0)\n",
        "    if idx is None or idx >= len(model.Embedding.weight):\n",
        "        return None\n",
        "    return model.Embedding.weight[idx].detach()\n",
        "\n",
        "# --- Function to find similar words\n",
        "def find_similar_words(word, top_n=5):\n",
        "    vector = get_vector(word)\n",
        "    if vector is None:\n",
        "        return []\n",
        "    all_vectors = model.Embedding.weight.detach()\n",
        "    similarities = F.cosine_similarity(vector.unsqueeze(0), all_vectors)\n",
        "    top_indices = torch.topk(similarities, top_n + 1).indices.tolist()\n",
        "    similar_words = [\n",
        "        (id2word[i], round(float(similarities[i].item()), 4))\n",
        "        for i in top_indices if id2word[i] != word\n",
        "    ][:top_n]\n",
        "    return similar_words\n",
        "\n",
        "# --- Streamlit UI\n",
        "st.title(\"🔍 Word Similarity Explorer\")\n",
        "st.write(\"Enter a word to find semantically similar terms using your Word2Vec model.\")\n",
        "\n",
        "# Dropdown or input\n",
        "selected_word = st.selectbox(\"Choose a word:\", sorted(list(word2id.keys())))\n",
        "top_n = st.slider(\"Number of similar words:\", min_value=1, max_value=20, value=5)\n",
        "\n",
        "if st.button(\"Find Similar Words\"):\n",
        "    similar = find_similar_words(selected_word, top_n=top_n)\n",
        "    if similar:\n",
        "        st.success(\"Top similar words:\")\n",
        "        for w, score in similar:\n",
        "            st.write(f\"**{w}** — Similarity: `{score * 100:.2f}%`\")\n",
        "    else:\n",
        "        st.warning(\"Word not found or no similar words available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kude_Cxbkrps",
        "outputId": "6d715267-3c0d-4a75-9502-d94423f45bc2"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-03 07:27:01.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.545 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-03 07:27:01.560 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"cbow_weights.pth\")"
      ],
      "metadata": {
        "id": "G2CUNiUwJd-c"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bChwYSJyJd8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}