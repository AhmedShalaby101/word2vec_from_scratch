# Word2Vec From Scratch ğŸ§ ğŸ’¬

This project implements a Word2Vec model (CBOW architecture) **from scratch using PyTorch**. It includes a full training pipeline, dataset preprocessing, and a simple app to visualize word embeddings.

## ğŸš€ Features
- Custom implementation of Word2Vec (CBOW)
- PyTorch model training and saving
- Data preparation pipeline (raw â†’ processed)
- Embedding visualization interface via `APP.py`
- Modular project structure for easy experimentation

## ğŸ“ Project Structure

```
word2vec_from_scratch/
â”œâ”€â”€ APP.py                  # Front-end interface to explore embeddings
â”œâ”€â”€ Train.py                # Script to train the model
â”œâ”€â”€ word2vec_model.pth      # Saved PyTorch model
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # Project README
â”œâ”€â”€ app_screen_shots/       # Screenshots for app usage
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw datasets
â”‚   â”œâ”€â”€ processed/          # Processed training/validation sets
â”‚   â”œâ”€â”€ interim/            # Intermediate mappings (word2id, id2word)
â”‚   â””â”€â”€ external/           # External or third-party data (empty)
â””â”€â”€ docs/                   # Reserved for documentation
```

## ğŸ“¸ Screenshots

App interface samples (from `app_screen_shots/`):

![screenshot1]("word2vec_from_scratch/app_screen_shots/1.png")
![screenshot2](app_screen_shots/2.png)
![screenshot3](app_screen_shots/3.png)
![screenshot4](app_screen_shots/4.png)
![screenshot5](app_screen_shots/5.png)

## ğŸ”§ Installation

```bash
git clone https://github.com/yourusername/word2vec_from_scratch.git
cd word2vec_from_scratch
pip install -r requirements.txt
```

## ğŸ Running the Project
```bash
 > âš ï¸ you need to edit config file and constant file before running train.py
```
### Train the model

```bash
python Train.py
```

### Launch the App (e.g., Streamlit)

```bash
python -m streamlit run App.py

> âš ï¸ You may need to adjust the paths in the scripts depending on your working directory.
> âš ï¸ You may need to adjust the tokenizer function to use a different dataset

## ğŸ“Š Model

- Architecture: CBOW Word2Vec
- Framework: PyTorch
- EMBED_DIMENSION, window size(CBOW_N_WORDS),MIN_WORD_FREQUENCY,MAX_SEQUENCE_LENGTH  â€“ configurable in `config.py`

## ğŸ“‚ Dataset

- Preprocessed text data is located in `data/processed`
- Raw text file: `data/raw/raw_dataset.pkl`
- Vocabulary mappings: `data/interim/word2id`, `id2word`

## âœ… TODOs / Future Improvements

- Add skip-gram option
- Visualize embeddings with t-SNE or PCA
- Add evaluation metrics

## ğŸ¤ Notes:



## ğŸ“„ License

MIT License

---

*Made with â¤ï¸ by [Ahmed Shalaby]*
